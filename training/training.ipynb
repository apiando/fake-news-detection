{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab41824",
   "metadata": {},
   "source": [
    "***FAKE NEWS DETECTION***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79416529",
   "metadata": {},
   "source": [
    "**DATASET AND SPLIT**\n",
    "\n",
    "1. Load the fake and true csv files \n",
    "2. Label each dataset (0 for fake, 1 for true)\n",
    "3. Combines them into one data set but shuffle rows to ensure random distribution\n",
    "4. Split train 60%, validation 20%, and test 20%\n",
    "5. Save after split sizes and class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f53e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load datasets\n",
    "fake_df = pd.read_csv('Fake.csv')\n",
    "true_df = pd.read_csv('True.csv')\n",
    "\n",
    "# Add a label column to each dataset\n",
    "fake_df['label'] = 0  # 0 for fake news\n",
    "true_df['label'] = 1  # 1 for true news\n",
    "\n",
    "# Combine datasets\n",
    "combined_df = pd.concat([fake_df, true_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the dataset\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split into train+val and test sets (80/20 split)\n",
    "train_val_df, test_df = train_test_split(\n",
    "    combined_df, \n",
    "    test_size=0.2,  # 20% for test set\n",
    "    random_state=42,\n",
    "    stratify=combined_df['label']  # This ensures same proportion of true/fake in each split\n",
    ")\n",
    "\n",
    "# Step 6: Split train+val into train and validation sets (75/25 split, resulting in 60/20/20 overall)\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.25,  # 25% of 80% = 20% overall for validation\n",
    "    random_state=42,\n",
    "    stratify=train_val_df['label']\n",
    ")\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Total dataset size: {len(combined_df)}\")\n",
    "print(f\"Training set size: {len(train_df)} ({len(train_df)/len(combined_df)*100:.1f}%)\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(val_df)/len(combined_df)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(test_df)} ({len(test_df)/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Check class balance in each split\n",
    "print(\"\\nClass balance (percentage of true news):\")\n",
    "print(f\"Original data: {combined_df['label'].mean()*100:.1f}%\")\n",
    "print(f\"Training set: {train_df['label'].mean()*100:.1f}%\")\n",
    "print(f\"Validation set: {val_df['label'].mean()*100:.1f}%\")\n",
    "print(f\"Test set: {test_df['label'].mean()*100:.1f}%\")\n",
    "\n",
    "# Save the splits to CSV files\n",
    "train_df.to_csv('train.csv', index=False)\n",
    "val_df.to_csv('val.csv', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7f2295",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Initialize Hugging Face login\n",
    "login(\"HUGGING FACE API KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Verify label distribution\n",
    "print(\"Train distribution:\\n\", train_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nValidation distribution:\\n\", val_df['label'].value_counts(normalize=True))\n",
    "print(\"\\nTest distribution:\\n\", test_df['label'].value_counts(normalize=True))\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF + Logistic Regression as a stronger baseline\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train = tfidf.fit_transform(train_df['text'])\n",
    "X_val = tfidf.transform(val_df['text'])\n",
    "\n",
    "# Baselines\n",
    "baselines = {\n",
    "    \"majority\": DummyClassifier(strategy=\"most_frequent\"),\n",
    "    \"random\": DummyClassifier(strategy=\"uniform\"),\n",
    "    \"tfidf_lr\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "for name, model in baselines.items():\n",
    "    model.fit(X_train, train_df['label'])\n",
    "    preds = model.predict(X_val)\n",
    "    acc = accuracy_score(val_df['label'], preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(val_df['label'], preds, average='binary')\n",
    "    print(f\"{name.capitalize()} - Acc: {acc:.4f} | Prec: {prec:.4f} | Rec: {rec:.4f} | F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0fdc0",
   "metadata": {},
   "source": [
    "***ROBERTA MODEL***\n",
    "\n",
    "Robustly optimized BERT training approach (RoBERTa) was introduced by researchers at Facebook. This model is an improvised version of BERT, where it is trained using larger datasets, using high computational power. This new implementation will replaces BERT's static masking with dynamic pattern generation.\n",
    "Since RoBERTa is pre-trained models, they accept input only in a certain format, vectors of integers, where each integer value represents a token. They accept the input sequences and converts them into the required formats as needed by the models. To avoid the problem of overfitting, I have decided to train the model for just 2 epoch for the model for optimal results. Data Collator will pad data so that all examples are the same input length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Tokenize\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove non-input columns (AFTER tokenization)\n",
    "columns_to_remove = [\"title\", \"subject\", \"date\"]  # KEEP 'text' until tokenized\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "val_dataset = val_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Load pre-trained RoBERTa model for binary classification\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Create a data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually run a forward pass on one batch to test\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sample_loader = DataLoader(train_dataset, batch_size=2)\n",
    "batch = next(iter(sample_loader))\n",
    "\n",
    "# Move to same device as model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"])\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",  # Or any other model\n",
    "    num_labels=2          # Number of classes\n",
    ")\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             \n",
    "    num_train_epochs=1,                 # 2-3 epochs is the best\n",
    "    per_device_train_batch_size=4,     # Smaller batch size here but you should use 8\n",
    "    learning_rate=5e-5,                 # Slightly higher learning rate here but starting point should be 2e-5\n",
    "    max_steps=1000,                     # Limit total training steps (optional because we wanted to minimize running time)\n",
    "    logging_steps=100,                  # Log less frequently (prevent crashing runtime) \n",
    "    evaluation_strategy=\"epoch\",        # Evaluate once per epoch\n",
    "    save_strategy=\"epoch\",              # Save once per epoch\n",
    "    load_best_model_at_end=True,        # Early stopping (with callback)\n",
    "    # weight_decay=0.05,                 # Set this for regularization\n",
    "    # per_device_eval_batch_size=32,        # CRITICAL!!!)\n",
    ")\n",
    "\n",
    "# Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for confusion matrix\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b5208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words(model, tokenizer, n=20):\n",
    "    # Get weights of the classifier layer\n",
    "    weights = model.classifier.dense.weight.detach().numpy()\n",
    "    avg_weights = np.mean(weights, axis=0)\n",
    "    # Get token ids sorted by importance\n",
    "    top_indices = np.argsort(avg_weights)[-n:]\n",
    "    # Get corresponding tokens\n",
    "    top_tokens = [tokenizer.decode([i]) for i in top_indices]\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=avg_weights[top_indices], y=top_tokens)\n",
    "    plt.title(f\"Top {n} Important Tokens\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125337fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_dataset[\"label\"], predicted_labels)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=['Fake', 'Real'], \n",
    "           yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - RoBERTa')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58997a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"HUGGING FACE PATH\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
