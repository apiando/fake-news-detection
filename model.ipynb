{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb1373",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets accelerate\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827af034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327f97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.read_csv(\"True.csv\")\n",
    "fake_df = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "true_df[\"label\"] = 1\n",
    "fake_df[\"label\"] = 0\n",
    "\n",
    "df = pd.concat([true_df, fake_df], axis=0)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df[[\"title\", \"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3151af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.sample(1000)  # Sample 1000 rows from df\n",
    "data = data.drop(columns=[\"text\"])  # Drop the 'text' column from the sampled data\n",
    "data.sample(10)  # Display a random sample of 10 rows from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630cb15",
   "metadata": {},
   "source": [
    "Load and Label Data:\n",
    "- We are loading both real and fake news datasets\n",
    "- Adds a label column: 1 for true, 0 for fake\n",
    "- Combines and shuffles the dataset\n",
    "- Extracts the news the title and labels as Python lists\n",
    "- Splits into training and validation sets in this case we are doing 80 training/20 validation split (line 18). Giving 80% ensures the model sees diverse examples and 20% prevents overfitting (preventing that when model memorizes but fails in real. 50/50 too little 90/10 not reliable for small datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2)\n",
    "train_df = pd.DataFrame({\"text\": train_texts, \"label\": train_labels})\n",
    "val_df = pd.DataFrame({\"text\": val_texts, \"label\": val_labels})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1382ea35",
   "metadata": {},
   "source": [
    "Tokenization\n",
    "- We are loading BERT tokenizer that converts raw text into input IDs and attention masks so that BERT can understand. This is a pretrained BERT tokenizer.\n",
    "- Adds padding and truncates to 512 tokens max (BERT's limit).\n",
    "- Wraps the inputs and labels into datatsets that Trainer can understand. All fields must be lists of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df72013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    #example grabs the \"text\" field from each example in dataset. \n",
    "    # \"padding\" pads all sequences to the max length, and then truncates any \n",
    "    # sequences longer than the max length.\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "#first two line applies tokenize_function for every example in the dataset and instead of \n",
    "# processing one record at a time, this line of code runs them in batches\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = train_dataset.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "val_dataset = val_dataset.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "#BERT doesn't need raw text during training - it uses token IDs. \n",
    "train_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "#tokenize and ready for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae454fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4, # reduced from 8 to 4 to reduce local resource usage\n",
    "    per_device_eval_batch_size=4, # reduced from 8 to 4\n",
    "    num_train_epochs=2, # reduced from 3 to 2\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42951475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d728e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation/test set NOOOOOOOOO\n",
    "output = trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "# Predict on new data\n",
    "trainer.predict(test_dataset)\n",
    "\n",
    "# Save model manually (optional)\n",
    "trainer.save_model(\"my_saved_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222073a",
   "metadata": {},
   "source": [
    "Save pretrained model for implementation in web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a419c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my_saved_model\")\n",
    "tokenizer.save_pretrained(\"my_saved_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
